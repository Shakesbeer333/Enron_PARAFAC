# PARAFAC decomposition on the Enron E-Mail data set

Disclaimer: For the original idea of this project see
[Bader B.W., Berry M.W., Browne M. (2008) Discussion Tracking in Enron Email Using PARAFAC. In: Berry M.W., Castellanos M. (eds) Survey of Text Mining II. Springer, London.](https://doi.org/10.1007/978-1-84800-046-9_8)

Texts present an alternative perspective on a subject of interest (Acosta (2015)).  Unlike quantitative data, i.e.  numbers, texts are generally edited by a human which makesqualitative data diverse (Acosta (2015)).  Each document is individual as both styleand content of the document convey the author’s mindset (Acosta (2015)).  On the contrary, numbers communicate an objective perception.  Moreover, some information isonly available in a qualitative format. For instance, a time series of email conversationsmight reflect a decision making process of an institution, whereas numbers fail to doso (Acosta (2015)).  In some cases, qualitative data contain more valuable informationthan  structured  data.   According  to  Kloptchenko  et  al.  (2004),  ”the  textual  part  of an annual [company] report contains richer information than financial ratios.” Hence,qualitative data can provide new insights into the functioning of complex structuressuch as companies or social networks (Acosta (2015)).

The issue that comes along with unstructured data is the large amount of data.  Key performance indicators can usually be expressed with two numbers, whereas it takes several words to describe the performance of a company.  In order to facilitate interpretation of textual data, it is essential to reduce these large data volumes to a manageable size (Bader et al. (2007)).  Whenever a type of relationship can be expressed as a (high-dimensional) matrix,  e.g.  term frequencies in documents,  performing decomposition allows to detect latent structures in the underlying data (Rabanser et al. (2017)).  The challenge is to determine thetrue  influencing  factorsthat account for the empiricaldata relationships (Harshman (1970)).

The extent to which standard data analysis methods such as Singular Value Decomposition (SVD) offer explanatory power has always been controversial (Harshman (1970)). If a two-dimensional representation of data shares more than one latent variable, the seclassical models tolerate multiple sets of influencing factors (Harshman (1970); Acaret al. (2005)). All possible factor combinations are mathematically equivalent withinthe model (Harshman (1970)).  However, all valid solutions result in a different interpretation of the explanatory variables of the underlying data relationships (Harshman(1970)).  Literature states this issue asrotation  problem (Bro (1997)).  On the otherhand,  one  could  ask  why  this  is  a  problem  in  real  applications. It is  certainly  rea-sonable to argue, that any complex system can be defined in several ways (Harshman(1970)).  Harshman (1970) points out that ”explanatory descriptions have implicationsbeyond  the  current  set  of  measurements  being  described. Explanatory descriptions imply predictions about the results of other possible experiments.” Thus, explanatoryvariables which equally well fit a particular set of observations are most likely not thetrue influencing factorsof the population (Harshman (1970)). 

Research in numerous areas, including neuroscience, process analysis, social networksand text-mining confirm that two-dimensional data analysis techniques do not accurately  capture  the  latent  profiles  of  the  data  i.e. uniquelyidentify  the  underlying information content (Acar and Yener (2009)).  For example, Acar et al. (2005) demon-strate that SVD cannot completely detect the multilinear structure which is present inchatroom communication data as it might be noisy and multidimensional.  The problem does not arise from the data decomposition techniques themselves but rather fromthe low dimension of the data onto which the two-way analysis methods are applied. Acar and Yener (2009) argue that ”matrices are often not enough to represent all theinformation content of the data.” By contrast,  multiway data analysis tools like the  Parallel Factor (PARAFAC) method enable to discover the latent structure in higher-dimensional data - with the advantage of robustness to noise andunique decomposition(Acar and Yener (2009)). Literature refers to these multidimensional data structures as tensors(Rabanser et al. (2017)).

In this work,  I apply the PARAFAC decomposition on the publicly released Enron email dataset.  The objective is to uncover meaningful discussion threads in the email network  over  time.   To  the  best  of  my  knowledge,  Bader  et  al.  (2008)  are  the  firstwho apply a generalization of SDV on an email corpus, namley the email conversations within Enron (Acar et al. (2005)).  My work replicates Bader et al. (2008)’s procedureto get a better understanding of extracting latent structures in a set of unstructured documents i.e.  emails (Bader et al. (2008)). I approach the PARAFAC decomposition as follows.  First, I filter for emails written in  2001  which  is  the  year  when  Enron  filed  for  bankruptcy  (Bader  et  al.  (2008)). Moreover, I only consider email addresses which have been identified by Priebe et al.(2015) as worthwhile to investigate in.  Next, I break down the body of the email intoweighted word frequencies.  I omit certain terms to focus on the meaningful context.From the remaining content,  I create a data-cube with thethree  dimensions  author,time  and term allowing for aunique decompositionin the subsequent step.  Applying the PARAFAC tool provided by Kossaifi et al. (2019) on this three-dimensional tensor decomposes  the  email  corpus  of  Enron  into  14  threads  of  discussion.   The  ten  mostdominant words of each conversation, i.e.  words having the largest weighting, can thenbe  used  to  define  a  topic  for  the  corresponding  discussion.   Additionally,  the  result allows to track the discussions’ intensity over time.

I identify two of 14 conversation topics as meaningful, namely Law & Regulation and California.  For instance, the dominant term mismanagement can be connected to the California energy crisis of 2000 and 2001 during which Enron generated huge profit (Eichenwald and Richtel (2002)).  The latent word Monterey links to California as it is a city of this state. Montereyseems to be a gateway for Enron’s interests in California.Of the remaining twelve conversations only particular latent terms such as gasoline link to activities that Enron had been involved in.  These poor results might be due to noise in the data.  Further investigation reveal for example that single emails are written in polish  as  the  latent  term serdeczn suggests.   In  comparison,  Bader  et  al.  (2008)  identify eight of 25 discussions as expressive.  From these observations I conclude the following.  First, preprocessing the data is a crucial step for decomposing unstructured documents.  The human-generated stopword list of Bader et al. (2008) contains more than 47k words,  whereas mine consists of approximately 200 stopwords provided bythe  Natural  Language  Toolkit  (NLTK)  and  17  Regular  Expressions  (RegEx)  (Birdet  al.  (2017)).   This  difference  in  the  size  of  the  stopword  list  could  be  a  reason  for the  noise  in  my  data  and  consequently  the  poor  results  in  comparison  with  Baderet al. (2008).  Second, human intelligence is still necessary to interpret the connections between  dominant  terms  and  an  overarching  topic.   With  regards  to  preprocessing, human knowledge is also essential in identifying ”words with no specific reference toan Enron-related person or activity” in order to set up a stopword list (Bader et al.(2008)).

Link to data set: http://cis.jhu.edu/~parky/Enron/enron.html

