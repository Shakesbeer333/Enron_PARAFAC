# PARAFAC decomposition on the Enron E-Mail data set

Texts present an alternative perspective on a subject of interest (Acosta (2015)).  Un-like quantitative data, i.e.  numbers, texts are generally edited by a human which makesqualitative data diverse (Acosta (2015)).  Each document is individual as both styleand content of the document convey the author’s mindset (Acosta (2015)).  On the con-trary, numbers communicate an objective perception.  Moreover, some information isonly available in a qualitative format.  For instance, a time series of email conversationsmight reflect a decision making process of an institution, whereas numbers fail to doso (Acosta (2015)).  In some cases, qualitative data contain more valuable informationthan  structured  data.   According  to  Kloptchenko  et  al.  (2004),  ”the  textual  part  ofan annual [company] report contains richer information than financial ratios.” Hence,qualitative data can provide new insights into the functioning of complex structuressuch as companies or social networks (Acosta (2015)).
The issue that comes along with unstructured data is the large amount of data.  Keyperformance indicators can usually be expressed with two numbers, whereas it takesseveral words to describe the performance of a company.  In order to facilitate interpre-tation of textual data, it is essential to reduce these large data volumes to a manageablesize (Bader et al. (2007)).  Whenever a type of relationship can be expressed as a (high-dimensional) matrix,  e.g.  term frequencies in documents,  performing decompositionallows to detect latent structures in the underlying data (Rabanser et al. (2017)).  The challenge is to determine thetrue  influencing  factorsthat account for the empiricaldata relationships (Harshman (1970)).
The extent to which standard data analysis methods such as Singular Value Decompo-sition (SVD) offer explanatory power has always been controversial (Harshman (1970)).If a two-dimensional representation of data shares more than one latent variable, theseclassical models tolerate multiple sets of influencing factors (Harshman (1970); Acaret al. (2005)). All possible factor combinations are mathematically equivalent withinthe model (Harshman (1970)).  However, all valid solutions result in a different inter-pretation of the explanatory variables of the underlying data relationships (Harshman(1970)).  Literature states this issue asrotation  problem(Bro (1997)).  On the otherhand,  one  could  ask  why  this  is  a  problem  in  real  applications.   It  is  certainly  rea-sonable to argue, that any complex system can be defined in several ways (Harshman(1970)).  Harshman (1970) points out that ”explanatory descriptions have implicationsbeyond  the  current  set  of  measurements  being  described. Explanatory descriptions imply predictions about the results of other possible experiments.” Thus, explanatoryvariables which equally well fit a particular set of observations are most likely not thetrue influencing factorsof the population (Harshman (1970)). 
Research in numerous areas, including neuroscience, process analysis, social networksand text-mining confirm that two-dimensional data analysis techniques do not accu-rately  capture  the  latent  profiles  of  the  data  i.e.uniquelyidentify  the  underlyinginformation content (Acar and Yener (2009)).  For example, Acar et al. (2005) demon-strate that SVD cannot completely detect the multilinear structure which is present inchatroom communication data as it might be noisy and multidimensional1.  The prob-lem does not arise from the data decomposition techniques themselves but rather fromthe low dimension of the data onto which the two-way analysis methods are applied. Acar and Yener (2009) argue that ”matrices are often not enough to represent all theinformation content of the data.” By contrast,  multiway data analysis tools like theParallel Factor (PARAFAC) method enable to discover the latent structure in higher-dimensional data - with the advantage of robustness to noise andunique decomposition(Acar and Yener (2009)). Literature refers to these multidimensional data structuresastensors(Rabanser et al. (2017)).

In this thesis,  I apply the PARAFAC decomposition on the publicly released Enronemail dataset.  The objective is to uncover meaningful discussion threads in the emailnetwork  over  time.   To  the  best  of  my  knowledge,  Bader  et  al.  (2008)  are  the  firstwho apply a generalization of SDV on an email corpus, namley the email conversationswithin Enron (Acar et al. (2005)).  My work replicates Bader et al. (2008)’s procedureto get a better understanding of extracting latent structures in a set of unstructureddocuments i.e.  emails (Bader et al. (2008)).I approach the PARAFAC decomposition as follows.  First, I filter for emails writtenin  2001  which  is  the  year  when  Enron  filed  for  bankruptcy  (Bader  et  al.  (2008)).Moreover, I only consider email addresses which have been identified by Priebe et al.(2015) as worthwhile to investigate in.  Next, I break down the body of the email intoweighted word frequencies.  I omit certain terms to focus on the meaningful context.From the remaining content,  I create a data-cube with thethree  dimensions  author,time  and term allowing for aunique decompositionin the subsequent step.  Applying the PARAFAC tool provided by Kossaifi et al. (2019) on this three-dimensional tensor decomposes  the  email  corpus  of  Enron  into  14  threads  of  discussion.   The  ten  mostdominant words of each conversation, i.e.  words having the largest weighting, can thenbe  used  to  define  a  topic  for  the  corresponding  discussion.   Additionally,  the  result allows to track the discussions’ intensity over time.
I identify two of 14 conversation topics as meaningful, namelyLaw & RegulationandCalifornia.  For instance, the dominant termmismanagementcan be connected to the California energy crisis of 2000 and 2001 during which Enron generated huge profit (Eichenwald and Richtel (2002)).  The latent wordMontereylinks toCaliforniaas it isa city of this state.Montereyseems to be a gateway for Enron’s interests inCalifornia.Of the remaining twelve conversations only particular latent terms such asgasolinelinkto activities that Enron had been involved in.  These poor results might be due to noisein the data.  Further investigation reveal for example that single emails are written inpolish  as  the  latent  termserdeczn[ie]suggests.   In  comparison,  Bader  et  al.  (2008)  identify eight of 25 discussions as expressive.  From these observations I conclude thefollowing.  First, preprocessing the data is a crucial step for decomposing unstructureddocuments.  The human-generated stopword list of Bader et al. (2008) contains morethan 47kwords,  whereas mine consists of approximately 200 stopwords provided bythe  Natural  Language  Toolkit  (NLTK)  and  17  Regular  Expressions  (RegEx)  (Birdet  al.  (2017)).   This  difference  in  the  size  of  the  stopword  list  could  be  a  reason  forthe  noise  in  my  data  and  consequently  the  poor  results  in  comparison  with  Baderet al. (2008).  Second, human intelligence is still necessary to interpret the connectionsbetween  dominant  terms  and  an  overarching  topic.   With  regards  to  preprocessing,human knowledge is also essential in identifying ”words with no specific reference toan Enron-related person or activity” in order to set up a stopword list (Bader et al.(2008)).

Link to data set: http://cis.jhu.edu/~parky/Enron/enron.html
